{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd02fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of matches from HLTV given full content and 1 star fitler\n",
    "driver.get('https://www.hltv.org/results?content=stats&stars=1')\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "n = int(soup.find('span', {'class':'pagination-data'}).text.split()[-1])\n",
    "offset = range(0, n, 100)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not path.exists('matchIDs.csv'):\n",
    "\n",
    "    matchIDs = pd.DataFrame(columns=['Date', 'ID', 'Status'])\n",
    "\n",
    "    for page in offset:\n",
    "\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\n",
    "\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\n",
    "            date = date[12:]\n",
    "\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\n",
    "                ID = URL['href'].split('/')[2]\n",
    "                matchIDs = matchIDs.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\n",
    "\n",
    "        print(page)\n",
    "        sleeptime = random.uniform(1, 2)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    matchIDs = matchIDs[::-1].reset_index(drop = True)\n",
    "    matchIDs.to_csv('matchIDs.csv', index = False)\n",
    "\n",
    "else:\n",
    "    print('File already exists')\n",
    "    matchIDs = pd.read_csv('matchIDs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists('matchIDs.csv'):\n",
    "\n",
    "    harvest = pd.DataFrame()\n",
    "\n",
    "    for page in offset:\n",
    "\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\n",
    "        batch = pd.DataFrame(columns=['Date', 'ID', 'Status'])\n",
    "\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\n",
    "            date = date[12:]\n",
    "\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\n",
    "                ID = URL['href'].split('/')[2]\n",
    "                batch = batch.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\n",
    "        \n",
    "        booler = batch['ID'].isin(matchIDs['ID'])\n",
    "        batch = batch[~booler]\n",
    "        if sum(booler) == 100:\n",
    "            print('Break')\n",
    "            break\n",
    "\n",
    "        harvest = harvest.append(batch)\n",
    "        print(page)\n",
    "        sleeptime = random.uniform(1, 2)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    harvest = harvest[::-1]\n",
    "    matchIDs = matchIDs.append(harvest).reset_index(drop = True)\n",
    "    print(str(len(harvest)) + ' new matche(s) has/have been found and added')\n",
    "\n",
    "else:\n",
    "    print('File does not exist, initialize first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 out of 300\n",
      "2 out of 300\n",
      "3 out of 300\n",
      "4 out of 300\n",
      "5 out of 300\n",
      "6 out of 300\n",
      "7 out of 300\n",
      "8 out of 300\n",
      "9 out of 300\n",
      "10 out of 300\n",
      "11 out of 300\n",
      "12 out of 300\n",
      "13 out of 300\n",
      "14 out of 300\n",
      "15 out of 300\n",
      "16 out of 300\n",
      "17 out of 300\n",
      "18 out of 300\n",
      "19 out of 300\n",
      "20 out of 300\n",
      "21 out of 300\n",
      "22 out of 300\n",
      "23 out of 300\n",
      "24 out of 300\n",
      "25 out of 300\n",
      "26 out of 300\n",
      "27 out of 300\n",
      "28 out of 300\n",
      "29 out of 300\n",
      "30 out of 300\n",
      "31 out of 300\n",
      "32 out of 300\n",
      "33 out of 300\n",
      "34 out of 300\n",
      "35 out of 300\n",
      "36 out of 300\n",
      "37 out of 300\n",
      "38 out of 300\n",
      "39 out of 300\n",
      "40 out of 300\n",
      "41 out of 300\n",
      "42 out of 300\n",
      "43 out of 300\n",
      "44 out of 300\n",
      "45 out of 300\n",
      "46 out of 300\n",
      "47 out of 300\n",
      "48 out of 300\n",
      "49 out of 300\n",
      "50 out of 300\n",
      "51 out of 300\n",
      "52 out of 300\n",
      "53 out of 300\n",
      "54 out of 300\n",
      "55 out of 300\n",
      "56 out of 300\n",
      "57 out of 300\n",
      "58 out of 300\n",
      "59 out of 300\n",
      "60 out of 300\n",
      "61 out of 300\n",
      "62 out of 300\n",
      "63 out of 300\n",
      "64 out of 300\n",
      "65 out of 300\n",
      "66 out of 300\n",
      "67 out of 300\n",
      "68 out of 300\n",
      "69 out of 300\n",
      "70 out of 300\n",
      "71 out of 300\n",
      "72 out of 300\n",
      "73 out of 300\n",
      "74 out of 300\n",
      "75 out of 300\n",
      "76 out of 300\n",
      "77 out of 300\n",
      "78 out of 300\n",
      "79 out of 300\n",
      "80 out of 300\n",
      "81 out of 300\n",
      "82 out of 300\n",
      "83 out of 300\n",
      "84 out of 300\n",
      "85 out of 300\n",
      "86 out of 300\n",
      "87 out of 300\n",
      "88 out of 300\n",
      "89 out of 300\n",
      "90 out of 300\n",
      "91 out of 300\n",
      "92 out of 300\n",
      "93 out of 300\n",
      "94 out of 300\n",
      "95 out of 300\n",
      "96 out of 300\n",
      "97 out of 300\n",
      "98 out of 300\n",
      "99 out of 300\n",
      "100 out of 300\n",
      "101 out of 300\n",
      "102 out of 300\n",
      "103 out of 300\n",
      "104 out of 300\n",
      "105 out of 300\n",
      "106 out of 300\n",
      "107 out of 300\n",
      "108 out of 300\n",
      "109 out of 300\n",
      "110 out of 300\n",
      "111 out of 300\n",
      "112 out of 300\n",
      "113 out of 300\n",
      "114 out of 300\n",
      "115 out of 300\n",
      "116 out of 300\n",
      "117 out of 300\n",
      "118 out of 300\n",
      "119 out of 300\n",
      "120 out of 300\n",
      "121 out of 300\n",
      "122 out of 300\n",
      "123 out of 300\n",
      "124 out of 300\n",
      "125 out of 300\n",
      "126 out of 300\n",
      "127 out of 300\n",
      "128 out of 300\n",
      "129 out of 300\n",
      "130 out of 300\n",
      "131 out of 300\n",
      "132 out of 300\n",
      "133 out of 300\n",
      "134 out of 300\n",
      "135 out of 300\n",
      "136 out of 300\n",
      "137 out of 300\n",
      "138 out of 300\n",
      "139 out of 300\n",
      "140 out of 300\n",
      "141 out of 300\n",
      "142 out of 300\n",
      "143 out of 300\n",
      "144 out of 300\n",
      "145 out of 300\n",
      "146 out of 300\n",
      "147 out of 300\n",
      "148 out of 300\n",
      "149 out of 300\n",
      "150 out of 300\n",
      "151 out of 300\n",
      "152 out of 300\n",
      "153 out of 300\n",
      "154 out of 300\n",
      "155 out of 300\n",
      "156 out of 300\n",
      "157 out of 300\n",
      "158 out of 300\n",
      "159 out of 300\n",
      "160 out of 300\n",
      "161 out of 300\n",
      "162 out of 300\n",
      "163 out of 300\n",
      "164 out of 300\n",
      "165 out of 300\n",
      "166 out of 300\n",
      "167 out of 300\n",
      "168 out of 300\n",
      "169 out of 300\n",
      "170 out of 300\n",
      "171 out of 300\n",
      "172 out of 300\n",
      "173 out of 300\n",
      "174 out of 300\n",
      "175 out of 300\n",
      "176 out of 300\n",
      "177 out of 300\n",
      "178 out of 300\n",
      "179 out of 300\n",
      "180 out of 300\n",
      "181 out of 300\n",
      "182 out of 300\n",
      "183 out of 300\n",
      "184 out of 300\n",
      "185 out of 300\n",
      "186 out of 300\n",
      "187 out of 300\n",
      "188 out of 300\n",
      "189 out of 300\n",
      "190 out of 300\n",
      "191 out of 300\n",
      "192 out of 300\n",
      "193 out of 300\n",
      "194 out of 300\n",
      "195 out of 300\n",
      "196 out of 300\n",
      "197 out of 300\n",
      "198 out of 300\n",
      "199 out of 300\n",
      "200 out of 300\n",
      "201 out of 300\n",
      "202 out of 300\n",
      "203 out of 300\n",
      "204 out of 300\n",
      "205 out of 300\n",
      "206 out of 300\n",
      "207 out of 300\n",
      "208 out of 300\n",
      "209 out of 300\n",
      "210 out of 300\n",
      "211 out of 300\n",
      "212 out of 300\n",
      "213 out of 300\n",
      "214 out of 300\n",
      "215 out of 300\n",
      "216 out of 300\n",
      "217 out of 300\n",
      "218 out of 300\n",
      "219 out of 300\n",
      "220 out of 300\n",
      "221 out of 300\n",
      "222 out of 300\n",
      "223 out of 300\n",
      "224 out of 300\n",
      "225 out of 300\n",
      "226 out of 300\n",
      "227 out of 300\n",
      "228 out of 300\n",
      "229 out of 300\n",
      "230 out of 300\n",
      "231 out of 300\n",
      "232 out of 300\n",
      "233 out of 300\n",
      "234 out of 300\n",
      "235 out of 300\n",
      "236 out of 300\n",
      "237 out of 300\n",
      "238 out of 300\n",
      "239 out of 300\n",
      "240 out of 300\n",
      "241 out of 300\n",
      "242 out of 300\n",
      "243 out of 300\n",
      "244 out of 300\n",
      "245 out of 300\n",
      "246 out of 300\n",
      "247 out of 300\n",
      "248 out of 300\n",
      "249 out of 300\n",
      "250 out of 300\n",
      "251 out of 300\n",
      "252 out of 300\n",
      "253 out of 300\n",
      "254 out of 300\n",
      "255 out of 300\n",
      "256 out of 300\n",
      "257 out of 300\n",
      "258 out of 300\n",
      "259 out of 300\n",
      "260 out of 300\n",
      "261 out of 300\n",
      "262 out of 300\n",
      "263 out of 300\n",
      "264 out of 300\n",
      "265 out of 300\n",
      "266 out of 300\n",
      "267 out of 300\n",
      "268 out of 300\n",
      "269 out of 300\n",
      "270 out of 300\n",
      "271 out of 300\n",
      "272 out of 300\n",
      "273 out of 300\n",
      "274 out of 300\n",
      "275 out of 300\n",
      "276 out of 300\n",
      "277 out of 300\n",
      "278 out of 300\n",
      "279 out of 300\n",
      "280 out of 300\n",
      "281 out of 300\n",
      "282 out of 300\n",
      "283 out of 300\n",
      "284 out of 300\n",
      "285 out of 300\n",
      "286 out of 300\n",
      "287 out of 300\n",
      "288 out of 300\n",
      "289 out of 300\n",
      "290 out of 300\n",
      "291 out of 300\n",
      "292 out of 300\n",
      "293 out of 300\n",
      "294 out of 300\n",
      "295 out of 300\n",
      "296 out of 300\n",
      "297 out of 300\n",
      "298 out of 300\n",
      "299 out of 300\n",
      "300 out of 300\n",
      "Finished\n",
      "299 entries have been added\n",
      "9427 entries awaiting to be scraped\n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "maxiter = 300\n",
    "data = {}\n",
    "for row in matchIDs.itertuples():\n",
    "\n",
    "    if row.Status == 1 or row.Status == -1:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        driver.get('https://www.hltv.org/matches/' + str(row.ID) +'/anything')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        matchdata  = {}\n",
    "        matchdata['Date'] = row.Date\n",
    "        matchdata['Event'] = soup.find('div', {'class':'event text-ellipsis'}).text\n",
    "\n",
    "        team1info = soup.find('div', {'class': 'team1-gradient'})\n",
    "        team1name = team1info.find('div', {'class':'teamName'}).text\n",
    "        team1score = team1info.find('div', {'class':['won', 'lost', 'tie']}).text\n",
    "\n",
    "        team2info = soup.find('div', {'class': 'team2-gradient'})\n",
    "        team2name = team2info.find('div', {'class':'teamName'}).text\n",
    "        team2score = team2info.find('div', {'class':['won', 'lost', 'tie']}).text\n",
    "\n",
    "        teams = [team1name, team2name]\n",
    "        score = [team1score, team2score]\n",
    "        matchdata['Teams'] = teams\n",
    "        matchdata['Score'] = score\n",
    "\n",
    "        maps = []\n",
    "        mapinfo = soup.find('div', {'class':'flexbox nowrap'})\n",
    "        for m in mapinfo.findAll('div', {'class':'dynamic-map-name-full'})[1:]:\n",
    "            maps.append(m.text)\n",
    "        matchdata['Maps'] = maps\n",
    "\n",
    "        try:\n",
    "            oddsinfo = soup.find('div', {'class':'past-matches-grid'})\n",
    "            odds = oddsinfo.findAll('div', {'class':'past-matches-bottom-right-numbers'})\n",
    "            odds = [odds[0].text, odds[1].text]\n",
    "        except:\n",
    "            odds = 'No odds available'\n",
    "        \n",
    "        matchdata['Odds'] = odds\n",
    "\n",
    "        matchlinks = []\n",
    "        for div in soup.findAll('div', {'class':'results-center-stats'}):\n",
    "            for a in div.findAll('a'):\n",
    "                link = a['href']\n",
    "                matchlinks.append(link)\n",
    "\n",
    "        match = {}\n",
    "        count = 0\n",
    "        for link in matchlinks:\n",
    "\n",
    "            driver.get('https://www.hltv.org/' + str(link))\n",
    "            content = driver.page_source\n",
    "            soup = BeautifulSoup(content)\n",
    "\n",
    "            game_stats = {}\n",
    "            scoreboard = soup.findAll('table', {'class':'stats-table'})\n",
    "            team1 = pd.read_html(str(scoreboard))[0].values.tolist()\n",
    "            team2 = pd.read_html(str(scoreboard))[1].values.tolist()\n",
    "            scoreboard = {}\n",
    "            scoreboard['Team 1'] = team1\n",
    "            scoreboard['Team 2'] = team2\n",
    "            game_stats['Scoreboard'] = scoreboard\n",
    "\n",
    "            misc_stats = soup.findAll('div', attrs={'class': 'right'})\n",
    "            game_stats['Starting side'] = misc_stats[0].contents[4]['class'][0][:-6]\n",
    "            game_stats['Rounds'] = misc_stats[0].text\n",
    "            game_stats['Team rating'] = misc_stats[1].text\n",
    "            game_stats['Entries'] = misc_stats[2].text\n",
    "            game_stats['Clutches'] = misc_stats[3].text\n",
    "\n",
    "            count += 1\n",
    "            match['Game ' + str(count)] = game_stats\n",
    "\n",
    "            sleeptime = random.uniform(0, 1)\n",
    "            time.sleep(sleeptime)\n",
    "\n",
    "        matchdata['Match'] = match\n",
    "        data[str(row.ID)] = matchdata\n",
    "        matchIDs.loc[row.Index, 'Status'] = 1\n",
    "\n",
    "    except:\n",
    "        matchIDs.loc[row.Index, 'Status'] = -1\n",
    "\n",
    "    iterations +=1\n",
    "    print(str(iterations) + ' out of ' + str(maxiter))\n",
    "\n",
    "    if iterations == maxiter:\n",
    "        print('Finished')\n",
    "        break\n",
    "\n",
    "if path.exists('main.json'):\n",
    "    with open('main.json', 'r+') as f:\n",
    "        main = json.load(f)\n",
    "        main.update(data)\n",
    "        f.seek(0)\n",
    "        json.dump(main, f, indent = 1)\n",
    "        f.close()\n",
    "else:\n",
    "    j = json.dumps(data, indent = 1)\n",
    "    with open('main.json', 'w') as f:\n",
    "        f.write(j)\n",
    "        f.close()\n",
    "\n",
    "matchIDs.to_csv('matchIDs.csv', index = False)\n",
    "\n",
    "print(str(len(data)) + ' entries have been added')\n",
    "print(str(len(matchIDs.Status) - sum(matchIDs.Status)) + ' entries awaiting to be scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n1337\n1337\n"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "test = json.load(open('main.json'))\n",
    "print(len(test) == sum(matchIDs.Status == 1))\n",
    "print(len(test))\n",
    "print(sum(matchIDs.Status == 1))"
   ]
  }
 ]
}