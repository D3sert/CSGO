{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd02fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of matches from HLTV given full content and 1 star fitler\n",
    "driver.get('https://www.hltv.org/results?content=stats&stars=1')\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "n = int(soup.find('span', {'class':'pagination-data'}).text.split()[-1])\n",
    "offset = range(0, n, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not path.exists('matchIDs.csv'):\n",
    "\n",
    "    matchIDs = pd.DataFrame(columns=['Date', 'ID', 'Status'])\n",
    "\n",
    "    for page in offset:\n",
    "\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\n",
    "\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\n",
    "            date = date[12:]\n",
    "\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\n",
    "                ID = URL['href'].split('/')[2]\n",
    "                matchIDs = matchIDs.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\n",
    "\n",
    "        print(page)\n",
    "        sleeptime = random.uniform(1, 2)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    matchIDs = matchIDs[::-1].reset_index(drop = True)\n",
    "    matchIDs.to_csv('matchIDs.csv', index = False)\n",
    "\n",
    "else:\n",
    "    print('File already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchIDs = pd.read_csv('matchIDs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "matchIDs = matchIDs[:-148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists('matchIDs.csv'):\n",
    "\n",
    "    harvest = pd.DataFrame()\n",
    "\n",
    "    for page in offset:\n",
    "\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\n",
    "        batch = pd.DataFrame(columns=['Date', 'ID', 'Status'])\n",
    "\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\n",
    "            date = date[12:]\n",
    "\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\n",
    "                ID = URL['href'].split('/')[2]\n",
    "                batch = batch.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\n",
    "        \n",
    "        booler = batch['ID'].isin(matchIDs['ID'])\n",
    "        batch = batch[~booler]\n",
    "        if sum(booler) == 100:\n",
    "            print('Break')\n",
    "            break\n",
    "\n",
    "        harvest = harvest.append(batch)\n",
    "        print(page)\n",
    "        sleeptime = random.uniform(1, 2)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    harvest = harvest[::-1]\n",
    "    matchIDs = matchIDs.append(harvest).reset_index(drop = True)\n",
    "    print(str(len(harvest)) + ' new matches have been found and added')\n",
    "\n",
    "else:\n",
    "    print('File does not exist, initialize first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Flag status when scraped\n",
    "for row in matchIDs.itertuples():\n",
    "    print(row.Date, row.ID, row.Status)\n",
    "    matchIDs.loc[row.Index, 'Status'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIDs = matchIDs[:5]\n",
    "print(testIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 2298365 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.hltv.org/matches/' + str(ID) +'/anything')\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchdata  = {}\n",
    "matchdata['Date'] = 'October 1st 2015'\n",
    "matchdata['Event'] = soup.find('div', {'class':'event text-ellipsis'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1info = soup.find('div', {'class': 'team1-gradient'})\n",
    "team1name = team1info.find('div', {'class':'teamName'}).text\n",
    "team1score = team1info.find('div', {'class':['won', 'lost', 'tie']}).text\n",
    "\n",
    "team2info = soup.find('div', {'class': 'team2-gradient'})\n",
    "team2name = team2info.find('div', {'class':'teamName'}).text\n",
    "team2score = team2info.find('div', {'class':['won', 'lost', 'tie']}).text\n",
    "\n",
    "teams = [team1name, team2name]\n",
    "score = [team1score, team2score]\n",
    "print(teams)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps and Odds"
   ]
  }
 ]
}