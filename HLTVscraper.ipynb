{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd02fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10772\n"
     ]
    }
   ],
   "source": [
    "# Get number of matches from HLTV given full content and 1 star fitler\n",
    "driver.get('https://www.hltv.org/results?content=stats&stars=1')\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "n = int(soup.find('span', {'class':'pagination-data'}).text.split()[-1])\n",
    "offset = range(0, n, 100)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "if not path.exists('matchIDs.csv'):\n",
    "\n",
    "    matchIDs = pd.DataFrame(columns=['Date', 'ID', 'Status'])\n",
    "\n",
    "    for page in offset:\n",
    "\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\n",
    "\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\n",
    "            date = date[12:]\n",
    "\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\n",
    "                ID = URL['href'].split('/')[2]\n",
    "                matchIDs = matchIDs.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\n",
    "\n",
    "        print(page)\n",
    "        sleeptime = random.uniform(1, 2)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    matchIDs = matchIDs[::-1].reset_index(drop = True)\n",
    "    matchIDs.to_csv('matchIDs.csv', index = False)\n",
    "\n",
    "else:\n",
    "    print('File already exists')\n",
    "    matchIDs = pd.read_csv('matchIDs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Break\n",
      "4 new matche(s) has/have been found and added\n"
     ]
    }
   ],
   "source": [
    "if path.exists('matchIDs.csv'):\n",
    "\n",
    "    harvest = pd.DataFrame()\n",
    "\n",
    "    for page in offset:\n",
    "\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\n",
    "        batch = pd.DataFrame(columns=['Date', 'ID', 'Status'])\n",
    "\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\n",
    "            date = date[12:]\n",
    "\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\n",
    "                ID = URL['href'].split('/')[2]\n",
    "                batch = batch.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\n",
    "        \n",
    "        booler = batch['ID'].isin(matchIDs['ID'])\n",
    "        batch = batch[~booler]\n",
    "        if sum(booler) == 100:\n",
    "            print('Break')\n",
    "            break\n",
    "\n",
    "        harvest = harvest.append(batch)\n",
    "        print(page)\n",
    "        sleeptime = random.uniform(1, 2)\n",
    "        time.sleep(sleeptime)\n",
    "\n",
    "    harvest = harvest[::-1]\n",
    "    matchIDs = matchIDs.append(harvest).reset_index(drop = True)\n",
    "    print(str(len(harvest)) + ' new matche(s) has/have been found and added')\n",
    "\n",
    "else:\n",
    "    print('File does not exist, initialize first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 out of 300\n",
      "2 out of 300\n",
      "3 out of 300\n",
      "4 out of 300\n",
      "Finished\n",
      "4 entries have been added\n",
      "0 entries awaiting to be scraped\n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "maxiter = 300\n",
    "data = {}\n",
    "for row in matchIDs.itertuples():\n",
    "\n",
    "    if row.Status == 1 or row.Status == -1:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        driver.get('https://www.hltv.org/matches/' + str(row.ID) +'/anything')\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        matchdata  = {}\n",
    "        matchdata['Date'] = row.Date\n",
    "        matchdata['Event'] = soup.find('div', {'class':'event text-ellipsis'}).text\n",
    "\n",
    "        team1info = soup.find('div', {'class': 'team1-gradient'})\n",
    "        team1name = team1info.find('div', {'class':'teamName'}).text\n",
    "        team1score = team1info.find('div', {'class':['won', 'lost', 'tie']}).text\n",
    "\n",
    "        team2info = soup.find('div', {'class': 'team2-gradient'})\n",
    "        team2name = team2info.find('div', {'class':'teamName'}).text\n",
    "        team2score = team2info.find('div', {'class':['won', 'lost', 'tie']}).text\n",
    "\n",
    "        teams = [team1name, team2name]\n",
    "        score = [team1score, team2score]\n",
    "        matchdata['Teams'] = teams\n",
    "        matchdata['Score'] = score\n",
    "\n",
    "        maps = []\n",
    "        mapinfo = soup.find('div', {'class':'flexbox nowrap'})\n",
    "        for m in mapinfo.findAll('div', {'class':'dynamic-map-name-full'})[1:]:\n",
    "            maps.append(m.text)\n",
    "        matchdata['Maps'] = maps\n",
    "\n",
    "        try:\n",
    "            oddsinfo = soup.find('div', {'class':'past-matches-grid'})\n",
    "            odds = oddsinfo.findAll('div', {'class':'past-matches-bottom-right-numbers'})\n",
    "            odds = [odds[0].text, odds[1].text]\n",
    "        except:\n",
    "            odds = 'No odds available'\n",
    "        \n",
    "        matchdata['Odds'] = odds\n",
    "\n",
    "        matchlinks = []\n",
    "        for div in soup.findAll('div', {'class':'results-center-stats'}):\n",
    "            for a in div.findAll('a'):\n",
    "                link = a['href']\n",
    "                matchlinks.append(link)\n",
    "\n",
    "        match = {}\n",
    "        count = 0\n",
    "        for link in matchlinks:\n",
    "\n",
    "            driver.get('https://www.hltv.org/' + str(link))\n",
    "            content = driver.page_source\n",
    "            soup = BeautifulSoup(content)\n",
    "\n",
    "            game_stats = {}\n",
    "            scoreboard = soup.findAll('table', {'class':'stats-table'})\n",
    "            team1 = pd.read_html(str(scoreboard))[0].values.tolist()\n",
    "            team2 = pd.read_html(str(scoreboard))[1].values.tolist()\n",
    "            scoreboard = {}\n",
    "            scoreboard['Team 1'] = team1\n",
    "            scoreboard['Team 2'] = team2\n",
    "            game_stats['Scoreboard'] = scoreboard\n",
    "\n",
    "            misc_stats = soup.findAll('div', attrs={'class': 'right'})\n",
    "            game_stats['Starting side'] = misc_stats[0].contents[4]['class'][0][:-6]\n",
    "            game_stats['Rounds'] = misc_stats[0].text\n",
    "            game_stats['Team rating'] = misc_stats[1].text\n",
    "            game_stats['Entries'] = misc_stats[2].text\n",
    "            game_stats['Clutches'] = misc_stats[3].text\n",
    "\n",
    "            count += 1\n",
    "            match['Game ' + str(count)] = game_stats\n",
    "\n",
    "            sleeptime = random.uniform(0, 1)\n",
    "            time.sleep(sleeptime)\n",
    "\n",
    "        matchdata['Match'] = match\n",
    "        data[str(row.ID)] = matchdata\n",
    "        matchIDs.loc[row.Index, 'Status'] = 1\n",
    "\n",
    "    except:\n",
    "        matchIDs.loc[row.Index, 'Status'] = -1\n",
    "\n",
    "    iterations +=1\n",
    "    print(str(iterations) + ' out of ' + str(maxiter))\n",
    "\n",
    "    if iterations == maxiter:\n",
    "        break\n",
    "\n",
    "print('Finished')\n",
    "\n",
    "if path.exists('main.json'):\n",
    "    with open('main.json', 'r+') as f:\n",
    "        main = json.load(f)\n",
    "        main.update(data)\n",
    "        f.seek(0)\n",
    "        json.dump(main, f, indent = 1)\n",
    "        f.close()\n",
    "else:\n",
    "    j = json.dumps(data, indent = 1)\n",
    "    with open('main.json', 'w') as f:\n",
    "        f.write(j)\n",
    "        f.close()\n",
    "\n",
    "matchIDs.to_csv('matchIDs.csv', index = False)\n",
    "\n",
    "print(str(len(data)) + ' entries have been added')\n",
    "print(str(sum(matchIDs.Status == 0)) + ' entries awaiting to be scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n10756\n10756\n"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "test = json.load(open('main.json'))\n",
    "print(len(test) == sum(matchIDs.Status == 1))\n",
    "print(len(test))\n",
    "print(sum(matchIDs.Status == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 364
    }
   ],
   "source": [
    "# Botched\n",
    "sum(matchIDs.Status == -1)"
   ]
  }
 ]
}