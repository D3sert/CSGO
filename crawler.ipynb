{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0812300918a79f3a093ad4f70c99c27e1a9e367741fa0491771d6b021d3e0db15",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver\\chromedriver.exe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##HELPERFUNCTIONS\n",
    "\n",
    "def get_matchLinks(page):\n",
    "    matchesLinks = []\n",
    "    ## https://www.hltv.org/results?offset=0\n",
    "    driver.get('https://www.hltv.org/results?offset='+ str(page) +'&stars=1')\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    ## FIND MATCHES\n",
    "    for div in soup.findAll('div', attrs={'class': 'results'}):\n",
    "        for a in div.findAll('a', attrs={'class': 'a-reset'}):\n",
    "            link = a['href']\n",
    "            if (link[:8] == \"/matches\"):\n",
    "                matchesLinks.append(link)\n",
    "    return matchesLinks\n",
    "\n",
    "def get_page_data(link):\n",
    "    url = 'https://www.hltv.org/' + str(link)\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)    \n",
    "    return soup\n",
    "\n",
    "def get_game_links(soup):\n",
    "    statlinks = []\n",
    "    for div in soup.findAll('div', attrs={'class': 'results-center-stats'}):\n",
    "        for a in div.findAll('a'):\n",
    "            statlinks.append(a['href'])\n",
    "    return statlinks\n",
    "\n",
    "def get_game_stats(link):\n",
    "    result = get_page_data(link)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GAME STAT INFO\n",
    "##GATHER PLAYER STATS\n",
    "\n",
    "#print(data[8][0])\n",
    "#LINK /stats/matches/mapstatsid/118030/astralis-vs-og\n",
    "def gather_player_stats(link):\n",
    "    colnames = [\"Player\",\"K (hs)\",\"A (f)\",\"D\",\"KAST\",\"K-D Diff\",\"ADR\",\"FK Diff\",\"Rating\",\"Team\"]\n",
    "\n",
    "    #localLink = data[8][0]\n",
    "    localLink = link\n",
    "    result = get_game_stats(localLink)\n",
    "    result = result.findAll('table', {'class':'stats-table'})\n",
    "    page_source = result\n",
    "    results = pd.read_html(str(result))\n",
    "\n",
    "    MapID = localLink.split(\"/\")[4]\n",
    "\n",
    "    #print(MapID)\n",
    "    #print(Map)\n",
    "    df_team1 = pd.DataFrame(results[0])\n",
    "    df_team1['Team'] = df_team1.columns[0]\n",
    "    df_team1.columns = colnames\n",
    "    #df_team1[\"Player\"].replace('\\r\\n',regex=True,inplace=True)\n",
    "\n",
    "    df_team2 = pd.DataFrame(results[1])\n",
    "    df_team2['Team'] = df_team2.columns[0]\n",
    "    df_team2.columns = colnames\n",
    "    #df_team2[\"Player\"].replace('\\r\\n',regex=True,inplace=True)\n",
    "\n",
    "    game_df = df_team1.append(df_team2,ignore_index=True)\n",
    "    game_df['MapID'] = MapID\n",
    "    #game_df['Map'] \n",
    "    #print(game_df)\n",
    "    #game_df.to_csv(index=True)\n",
    "    return game_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BASIC INFO\n",
    "\n",
    "def gather_match_stats(links):\n",
    "    match_df = pd.DataFrame()\n",
    "    ##GET DATE from https://www.hltv.org/matches/2347834***\n",
    "    names = [\"Date\",\"Tournament\",\"Team1\",\"Result1\",\"Team2\",\"Result2\",\"Map\",\"MapID\",\"StatLinks\"]\n",
    "    #names = [\"Date\",\"Tournament\",\"Team1\",\"Result1\",\"Team2\",\"Result2\"]\n",
    "    data = []\n",
    "    maps = []\n",
    "    mapsID = []\n",
    "    players = []\n",
    "\n",
    "    for div in soup.findAll('div', attrs={'class': 'match-page'}):\n",
    "        pageDate = div.find('div', attrs={'class': 'date'})\n",
    "        pageTournament = div.find('div', attrs={'class': 'event text-ellipsis'})\n",
    "        data.append(pageDate.text)\n",
    "        data.append(pageTournament.text)\n",
    "\n",
    "    ##GET FINALRESULT1 from https://www.hltv.org/matches/2347834***\n",
    "    for div in soup.findAll('div', attrs={'class': 'team1-gradient'}):\n",
    "        pageTeam1 = div.find('div', attrs={'class': 'teamName'})\n",
    "        pageResult1 = div.find('div', attrs={'class': ['won', 'lost', 'tie']})\n",
    "        data.append(pageTeam1.text)\n",
    "        data.append(pageResult1.text)\n",
    "\n",
    "    ##GET FINALRESULT2 from https://www.hltv.org/matches/2347834***\n",
    "    for div in soup.findAll('div', attrs={'class': 'team2-gradient'}):\n",
    "        pageTeam2 = div.find('div', attrs={'class': 'teamName'})\n",
    "        pageResult2 = div.find('div', attrs={'class': ['won', 'lost', 'tie']})\n",
    "        data.append(pageTeam2.text)\n",
    "        data.append(pageResult2.text)\n",
    "\n",
    "    ##GET MAPS AND MAPSID from https://www.hltv.org/matches/2347834***\n",
    "    for div in soup.findAll('div', attrs={'class': 'stats-menu-link'}):\n",
    "        map1 = div.findAll('div', attrs= {'class': 'dynamic-map-name-full'})\n",
    "        for div in map1:\n",
    "            maps.append(div.text)\n",
    "            mapsID.append(div['id'])\n",
    "\n",
    "    #map_df = pd.DataFrame(maps)\n",
    "    data.append(maps)\n",
    "    #mapsID_df = pd.DataFrame(mapsID)\n",
    "    data.append(mapsID)\n",
    "\n",
    "\n",
    "    ##GET DETAILED STATS LINKS\n",
    "    data.append(links)\n",
    "\n",
    "    ##GET PLAYER INFO\n",
    "    def get_player_info_def():\n",
    "        for div in soup.findAll('div', attrs={'class': 'lineups'}):\n",
    "            for table in div.findAll('td',attrs={'class': 'player player-image'}):\n",
    "                for a in table.findAll('a'):\n",
    "                    players.append(a['href'])\n",
    "        player_df = pd.DataFrame(players)\n",
    "        data_dict = {'data':data}\n",
    "        data_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    #data_df.index = names\n",
    "    #-----------\n",
    "    #maps_df = pd.DataFrame(data_df.iloc[[7]].values)\n",
    "    #print(data_df.iloc[[7]].values)\n",
    "    #print(data_df)\n",
    "    for i in range(1,len(maps)):\n",
    "        #print(maps[i])\n",
    "        #print(mapsID[i])\n",
    "        #print(links[i-1])\n",
    "        game_df = gather_player_stats(links[i-1])\n",
    "        game_df['Map'] = maps[i]\n",
    "        game_df['Tournament'] = data[1]\n",
    "        game_df['Date'] = data[0]\n",
    "        game_df['Team1'] = data[2]\n",
    "        game_df['Team2'] = data[4]\n",
    "        game_df['ResultTeam1'] = data[3]\n",
    "        game_df['ResultTeam2'] = data[5]\n",
    "        match_df = match_df.append(game_df)\n",
    "\n",
    "    return match_df\n",
    "    #match_df.to_clipboard()\n",
    "\n",
    "\n",
    "\n",
    "    #data_df.iloc[7].values[0][0] -> maps\n",
    "    #data_df.iloc[8].values[0][0] -> mapID\n",
    "    #data_df.iloc[9].values[0][0] -> players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "J: 0\n",
      "Stating at: 716 with 50 incs\n",
      "Fetching: 2343943 I: 0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'RunningTest.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-cc13c262b70e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MatchID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m#print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'RunningTest.csv'"
     ]
    }
   ],
   "source": [
    "##MAIN\n",
    "for j in range(0,20):   \n",
    "    print(\"J: \"+str(j))\n",
    "    df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "    total_matches = len(df_running['MatchID'].unique())\n",
    "    n_runs = 50\n",
    "    print(\"Stating at: \"+ str(total_matches)+ \" with \" + str(n_runs) + \" incs\")\n",
    "    \n",
    "    runname = \"RunningTest\"\n",
    "\n",
    "    #CAREFUL FETCHES PAGE (runtime=1)\n",
    "    matchesLinks = get_matchLinks(total_matches) #goes to results page to fetch matchLinks\n",
    "    #print(pd.DataFrame(matchesLinks))\n",
    "\n",
    "    for i in range(0,n_runs): #len(matchesLinks)\n",
    "        match_id = matchesLinks[i].split(\"/\")[2]\n",
    "        if len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) != 0:\n",
    "            print(\"Duplicate Skipping\" + match_id)\n",
    "        else:\n",
    "            print(\"Fetching: \" +str(match_id) + \" I: \"+ str(i))\n",
    "            #CAREFUL FETCHES PAGE\n",
    "            soup = get_page_data(matchesLinks[i]) #gets the soup for matchpage from match links\n",
    "            gamelinks = get_game_links(soup) #gets the gamelinks from match links\n",
    "            output = gather_match_stats(gamelinks)  #gets stats from gamelinks\n",
    "            output['MatchID'] = match_id\n",
    "            #print(output)\n",
    "            output.to_csv(str(runname)+'.csv', mode='a', header=False,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAIN\n",
    "for j in range(0,20):   \n",
    "    print(\"J: \"+str(j))\n",
    "    df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "    total_matches = len(df_running['MatchID'].unique())\n",
    "    n_runs = 50\n",
    "    print(\"Stating at: \"+ str(total_matches)+ \" with \" + str(n_runs) + \" incs\")\n",
    "    \n",
    "    runname = \"RunningTest\"\n",
    "\n",
    "    #CAREFUL FETCHES PAGE (runtime=1)\n",
    "    matchesLinks = get_matchLinks(total_matches) #goes to results page to fetch matchLinks\n",
    "    #print(pd.DataFrame(matchesLinks))\n",
    "\n",
    "    for i in range(0,n_runs): #len(matchesLinks)\n",
    "        match_id = matchesLinks[i].split(\"/\")[2]\n",
    "        if len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) != 0:\n",
    "            print(\"Duplicate Skipping\" + match_id)\n",
    "        else:\n",
    "            print(\"Fetching: \" +str(match_id) + \" I: \"+ str(i))\n",
    "            #CAREFUL FETCHES PAGE\n",
    "            soup = get_page_data(matchesLinks[i]) #gets the soup for matchpage from match links\n",
    "            gamelinks = get_game_links(soup) #gets the gamelinks from match links\n",
    "            output = gather_match_stats(gamelinks)  #gets stats from gamelinks\n",
    "            output['MatchID'] = match_id\n",
    "            #print(output)\n",
    "            output.to_csv(str(runname)+'.csv', mode='a', header=False,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stating at: 9\n",
      "Fetching: 2347024I: 0\n",
      "Fetching: 2347025I: 1\n",
      "Fetching: 2347738I: 2\n",
      "Fetching: 2347017I: 3\n",
      "Fetching: 2347016I: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "total_matches = len(df_running['MatchID'].unique())\n",
    "\n",
    "print(\"Stating at: \"+ str(total_matches))\n",
    "n_runs = 5\n",
    "runname = \"RunningTest\"\n",
    "\n",
    "#CAREFUL FETCHES PAGE (runtime=1)\n",
    "matchesLinks = get_matchLinks(total_matches) #goes to results page to fetch matchLinks\n",
    "#print(pd.DataFrame(matchesLinks))\n",
    "\n",
    "for i in range(0,n_runs): #len(matchesLinks)\n",
    "    match_id = matchesLinks[i].split(\"/\")[2]\n",
    "    if len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) != 0:\n",
    "        print(\"Duplicate Skipping\" + match_id)\n",
    "    else:\n",
    "        print(\"Fetching: \" +str(match_id) + \"I: \"+ str(i))\n",
    "        #CAREFUL FETCHES PAGE\n",
    "        soup = get_page_data(matchesLinks[i]) #gets the soup for matchpage from match links\n",
    "        gamelinks = get_game_links(soup) #gets the gamelinks from match links\n",
    "        output = gather_match_stats(gamelinks)  #gets stats from gamelinks\n",
    "        output['MatchID'] = match_id\n",
    "\n",
    "        #if(len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) == 0):\n",
    "        output.to_csv(str(runname)+'.csv', mode='a', header=False,index=False)\n",
    "\n",
    "        #output.to_csv('match_'+str(match_id)+'.csv',index=False)\n",
    "        #playersdf.to_csv('csplayers_nd_'+str(page)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "matchesLinks_ids = []\n",
    "for i in range(0,len(matchesLinks)):\n",
    "    matchesLinks_ids.append(matchesLinks[i].split(\"/\")[2])\n",
    "print(len(matchesLinks[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "df_running = pd.read_csv(\"run_Test_50.csv\")\n",
    "print(len(df_running['MatchID'].unique()))"
   ]
  },
  {
   "source": [
    "for i in range(0,len(results)):\n",
    "    print(results[i])\n",
    "df = pd.DataFrame(result[0])\n",
    "print(df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}