{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0812300918a79f3a093ad4f70c99c27e1a9e367741fa0491771d6b021d3e0db15",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver\\chromedriver.exe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "##HELPERFUNCTIONS\n",
    "\n",
    "def get_matchLinks(page):\n",
    "    matchesLinks = []\n",
    "    ## https://www.hltv.org/results?offset=0\n",
    "    driver.get('https://www.hltv.org/results?offset='+ str(page) +'&stars=1')\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    ## FIND MATCHES\n",
    "    for div in soup.findAll('div', attrs={'class': 'results'}):\n",
    "        for a in div.findAll('a', attrs={'class': 'a-reset'}):\n",
    "            link = a['href']\n",
    "            if (link[:8] == \"/matches\"):\n",
    "                matchesLinks.append(link)\n",
    "    return matchesLinks\n",
    "\n",
    "def get_page_data(link):\n",
    "    url = 'https://www.hltv.org/' + str(link)\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)    \n",
    "    return soup\n",
    "\n",
    "def get_game_links(soup):\n",
    "    statlinks = []\n",
    "    for div in soup.findAll('div', attrs={'class': 'results-center-stats'}):\n",
    "        for a in div.findAll('a'):\n",
    "            statlinks.append(a['href'])\n",
    "    return statlinks\n",
    "\n",
    "def get_game_stats(link):\n",
    "    result = get_page_data(link)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GAME STAT INFO\n",
    "##GATHER PLAYER STATS\n",
    "\n",
    "#print(data[8][0])\n",
    "#LINK /stats/matches/mapstatsid/118030/astralis-vs-og\n",
    "def gather_player_stats(link):\n",
    "    colnames = [\"Player\",\"K (hs)\",\"A (f)\",\"D\",\"KAST\",\"K-D Diff\",\"ADR\",\"FK Diff\",\"Rating\",\"Team\"]\n",
    "\n",
    "    #localLink = data[8][0]\n",
    "    localLink = link\n",
    "    result = get_game_stats(localLink)\n",
    "    result = result.findAll('table', {'class':'stats-table'})\n",
    "    page_source = result\n",
    "    results = pd.read_html(str(result))\n",
    "\n",
    "    MapID = localLink.split(\"/\")[4]\n",
    "\n",
    "    #print(MapID)\n",
    "    #print(Map)\n",
    "    df_team1 = pd.DataFrame(results[0])\n",
    "    df_team1['Team'] = df_team1.columns[0]\n",
    "    df_team1.columns = colnames\n",
    "    #df_team1[\"Player\"].replace('\\r\\n',regex=True,inplace=True)\n",
    "\n",
    "    df_team2 = pd.DataFrame(results[1])\n",
    "    df_team2['Team'] = df_team2.columns[0]\n",
    "    df_team2.columns = colnames\n",
    "    #df_team2[\"Player\"].replace('\\r\\n',regex=True,inplace=True)\n",
    "\n",
    "    game_df = df_team1.append(df_team2,ignore_index=True)\n",
    "    game_df['MapID'] = MapID\n",
    "    #game_df['Map'] \n",
    "    #print(game_df)\n",
    "    #game_df.to_csv(index=True)\n",
    "    return game_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BASIC INFO\n",
    "\n",
    "def gather_match_stats(links):\n",
    "    match_df = pd.DataFrame()\n",
    "    ##GET DATE from https://www.hltv.org/matches/2347834***\n",
    "    names = [\"Date\",\"Tournament\",\"Team1\",\"Result1\",\"Team2\",\"Result2\",\"Map\",\"MapID\",\"StatLinks\"]\n",
    "    #names = [\"Date\",\"Tournament\",\"Team1\",\"Result1\",\"Team2\",\"Result2\"]\n",
    "    data = []\n",
    "    maps = []\n",
    "    mapsID = []\n",
    "    players = []\n",
    "\n",
    "    for div in soup.findAll('div', attrs={'class': 'match-page'}):\n",
    "        pageDate = div.find('div', attrs={'class': 'date'})\n",
    "        pageTournament = div.find('div', attrs={'class': 'event text-ellipsis'})\n",
    "        data.append(pageDate.text)\n",
    "        data.append(pageTournament.text)\n",
    "\n",
    "    ##GET FINALRESULT1 from https://www.hltv.org/matches/2347834***\n",
    "    for div in soup.findAll('div', attrs={'class': 'team1-gradient'}):\n",
    "        pageTeam1 = div.find('div', attrs={'class': 'teamName'})\n",
    "        pageResult1 = div.find('div', attrs={'class': ['won', 'lost', 'tie']})\n",
    "        data.append(pageTeam1.text)\n",
    "        data.append(pageResult1.text)\n",
    "\n",
    "    ##GET FINALRESULT2 from https://www.hltv.org/matches/2347834***\n",
    "    for div in soup.findAll('div', attrs={'class': 'team2-gradient'}):\n",
    "        pageTeam2 = div.find('div', attrs={'class': 'teamName'})\n",
    "        pageResult2 = div.find('div', attrs={'class': ['won', 'lost', 'tie']})\n",
    "        data.append(pageTeam2.text)\n",
    "        data.append(pageResult2.text)\n",
    "\n",
    "    ##GET MAPS AND MAPSID from https://www.hltv.org/matches/2347834***\n",
    "    for div in soup.findAll('div', attrs={'class': 'stats-menu-link'}):\n",
    "        map1 = div.findAll('div', attrs= {'class': 'dynamic-map-name-full'})\n",
    "        for div in map1:\n",
    "            maps.append(div.text)\n",
    "            mapsID.append(div['id'])\n",
    "\n",
    "    #map_df = pd.DataFrame(maps)\n",
    "    data.append(maps)\n",
    "    #mapsID_df = pd.DataFrame(mapsID)\n",
    "    data.append(mapsID)\n",
    "\n",
    "\n",
    "    ##GET DETAILED STATS LINKS\n",
    "    data.append(links)\n",
    "\n",
    "    ##GET PLAYER INFO\n",
    "    def get_player_info_def:\n",
    "        for div in soup.findAll('div', attrs={'class': 'lineups'}):\n",
    "            for table in div.findAll('td',attrs={'class': 'player player-image'}):\n",
    "                for a in table.findAll('a'):\n",
    "                    players.append(a['href'])\n",
    "        player_df = pd.DataFrame(players)\n",
    "        data_dict = {'data':data}\n",
    "        data_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    #data_df.index = names\n",
    "    #-----------\n",
    "    #maps_df = pd.DataFrame(data_df.iloc[[7]].values)\n",
    "    #print(data_df.iloc[[7]].values)\n",
    "    #print(data_df)\n",
    "    for i in range(1,len(maps)):\n",
    "        #print(maps[i])\n",
    "        #print(mapsID[i])\n",
    "        #print(links[i-1])\n",
    "        game_df = gather_player_stats(links[i-1])\n",
    "        game_df['Map'] = maps[i]\n",
    "        game_df['Tournament'] = data[1]\n",
    "        game_df['Date'] = data[0]\n",
    "        game_df['Team1'] = data[2]\n",
    "        game_df['Team2'] = data[4]\n",
    "        game_df['ResultTeam1'] = data[3]\n",
    "        game_df['ResultTeam2'] = data[5]\n",
    "        match_df = match_df.append(game_df)\n",
    "\n",
    "    return match_df\n",
    "    #match_df.to_clipboard()\n",
    "\n",
    "\n",
    "\n",
    "    #data_df.iloc[7].values[0][0] -> maps\n",
    "    #data_df.iloc[8].values[0][0] -> mapID\n",
    "    #data_df.iloc[9].values[0][0] -> players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "J: 0\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 1\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 2\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 3\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 4\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 5\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 6\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 7\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 8\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 9\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 10\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 11\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 12\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 13\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 14\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 15\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 16\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 17\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 18\n",
      "Stating at: 214WITH 10 incs\n",
      "J: 19\n",
      "Stating at: 214WITH 10 incs\n"
     ]
    }
   ],
   "source": [
    "##MAIN\n",
    "for j in range(0,1):   \n",
    "    print(\"J: \"+str(j))\n",
    "        df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "        total_matches = len(df_running['MatchID'].unique())\n",
    "        n_runs = 2\n",
    "        print(\"Stating at: \"+ str(total_matches)+ \"WITH \" + str(n_runs) + \" incs\")\n",
    "        \n",
    "        runname = \"RunningTest\"\n",
    "\n",
    "        #CAREFUL FETCHES PAGE (runtime=1)\n",
    "        matchesLinks = get_matchLinks(total_matches) #goes to results page to fetch matchLinks\n",
    "        #print(pd.DataFrame(matchesLinks))\n",
    "\n",
    "        for i in range(0,n_runs): #len(matchesLinks)\n",
    "            match_id = matchesLinks[i].split(\"/\")[2]\n",
    "            if len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) != 0:\n",
    "                print(\"Duplicate Skipping\" + match_id)\n",
    "            else:\n",
    "                print(\"Fetching: \" +str(match_id) + \" I: \"+ str(i))\n",
    "                #CAREFUL FETCHES PAGE\n",
    "                soup = get_page_data(matchesLinks[i]) #gets the soup for matchpage from match links\n",
    "                gamelinks = get_game_links(soup) #gets the gamelinks from match links\n",
    "                output = gather_match_stats(gamelinks)  #gets stats from gamelinks\n",
    "                output['MatchID'] = match_id\n",
    "                print(output)\n",
    "                #output.to_csv(str(runname)+'.csv', mode='a', header=False,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stating at: 9\n",
      "Fetching: 2347024I: 0\n",
      "Fetching: 2347025I: 1\n",
      "Fetching: 2347738I: 2\n",
      "Fetching: 2347017I: 3\n",
      "Fetching: 2347016I: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_running = pd.read_csv(\"RunningTest.csv\")\n",
    "total_matches = len(df_running['MatchID'].unique())\n",
    "\n",
    "print(\"Stating at: \"+ str(total_matches))\n",
    "n_runs = 5\n",
    "runname = \"RunningTest\"\n",
    "\n",
    "#CAREFUL FETCHES PAGE (runtime=1)\n",
    "matchesLinks = get_matchLinks(total_matches) #goes to results page to fetch matchLinks\n",
    "#print(pd.DataFrame(matchesLinks))\n",
    "\n",
    "for i in range(0,n_runs): #len(matchesLinks)\n",
    "    match_id = matchesLinks[i].split(\"/\")[2]\n",
    "    if len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) != 0:\n",
    "        print(\"Duplicate Skipping\" + match_id)\n",
    "    else:\n",
    "        print(\"Fetching: \" +str(match_id) + \"I: \"+ str(i))\n",
    "        #CAREFUL FETCHES PAGE\n",
    "        soup = get_page_data(matchesLinks[i]) #gets the soup for matchpage from match links\n",
    "        gamelinks = get_game_links(soup) #gets the gamelinks from match links\n",
    "        output = gather_match_stats(gamelinks)  #gets stats from gamelinks\n",
    "        output['MatchID'] = match_id\n",
    "\n",
    "        #if(len(df_running['MatchID'].where(df_running['MatchID'] == match_id).dropna()) == 0):\n",
    "        output.to_csv(str(runname)+'.csv', mode='a', header=False,index=False)\n",
    "\n",
    "        #output.to_csv('match_'+str(match_id)+'.csv',index=False)\n",
    "        #playersdf.to_csv('csplayers_nd_'+str(page)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "matchesLinks_ids = []\n",
    "for i in range(0,len(matchesLinks)):\n",
    "    matchesLinks_ids.append(matchesLinks[i].split(\"/\")[2])\n",
    "print(len(matchesLinks[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "df_running = pd.read_csv(\"run_Test_50.csv\")\n",
    "print(len(df_running['MatchID'].unique()))"
   ]
  },
  {
   "source": [
    "for i in range(0,len(results)):\n",
    "    print(results[i])\n",
    "df = pd.DataFrame(result[0])\n",
    "print(df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}